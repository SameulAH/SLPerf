## 大致过程

比较简单, 客户端进行完一轮学习之后, 将模型发送到swapserver, 然后swapserver按照某种策略分发模型

## 详细过程

1. 各客户端进行正常进行一个epoch的学习

2. 各客户端对验证集完成验证之后, 把自己的模型参数上传至swapserver

3. swapserver按照一定方式, 将这些模型打乱重新发回客户端. 例如client1会收到client2上一次的模型, 而client2收到client3的模型, client3收到client1的模型

4. 回到1

## motivation

1. 对模型直接进行聚合并不能提升non-IID setting下的准确率(SplitFed), 很有可能是因为, 在non-IID的情况下, C1的模型学会了1,2,3的特征,而C2学会了4,5,6的特征, 直接进行平均会导致新的模型将1,2,3,4,5,6的特征都遗忘了.(可以通过对聚合后的模型直接进行验证而不学习来证明, 不过还没做这个实验)

2. 既然聚合对non-IID来说没有, 那么直接交换模型就成为了一个比较合理的选择, 因为这样, C1拿到了已经学会了4,5,6的模型, 现在只需要再利用自己1,2,3的训练集, 理论上就可以训练出1,2,3,4,5,6都会的模型了.

3. 然而这会导致一个新的问题, 灾难性遗忘, 也就是C1用新的模型学习完1,2,3之后, 会忘了4,5,6的特征, 所以根据日志也可以看出前几回合C1的准确率没什么提升, 虽然最后整体还是提升了, 但是收敛很慢而且不稳定.

4. 想要缓解这个问题, 可以加入一些新的机制比如知识蒸馏之类的. 

5. 或者想办法让模型交换的频繁一些, 比如学习几个batch就交换一次模型, 但不知道这样通讯成本会不会成为负担. 不过SL的优势之一也是Client的模型比较小, 所以感觉可以尝试.

6. 另外, 实际上多个客户端可以共享一个模型, 某个客户端想学习就直接下载, 学习完就上传, 一次只能有一个客户端在学习.

7. 有一个比较大的缺点是如果有一个客户端的数据集比较差, 或者有恶意的客户端, 就会让整个准确率降低很多(当然FL应该也有这种问题, 应该也是能解决的)

8. 
